# pakfro-ext README

This is a dry run of creating a vscode extension in Node.js and basic webdev stack that leverages a locally running llm, in this case "llama3.1:8b" (Llama 3.1 8 billion parameter by Meta)

I used a boilerplate from Yeoman and generator-code (npm
)
## Features

- Launches a debug window with the extension running
- calling the command runs the extension
- basic HTML/CSS/JS for prompt input and response from LLM


## Requirements
##### TODO:

- Make a decent looking GUI so the text doesn't appear in some hobo-style red box
- Severe lag between submitting user prompt --> LLM response. Why is it taking so long?
- try other models
